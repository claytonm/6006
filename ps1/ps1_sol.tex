%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,twoside]{article}
\usepackage[latin9]{luainputenc}
\usepackage{amsmath}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%
% 6.006 problem set 1
%




\input{macros}

\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}

% Fill these in!
\newcommand{\theproblemsetnum}{1}
\newcommand{\releasedate}{September 8, 2011}
\newcommand{\partaduedate}{Thursday, September 15}
\newcommand{\tabUnit}{3ex}
\newcommand{\tabT}{\hspace*{\tabUnit}}

\makeatother

\begin{document}
\handout{Problem Set \theproblemsetnum}{\releasedate}

\newif\ifsolution \solutiontrue \global\long\def\solution{\textbf{Your Solution:}}

\setlength{\parindent}{0pt}

\medskip{}

\hrulefill{}

\begin{problems}

\problem \points{15} \textbf{Asymptotic Practice}

For each group of functions, sort the functions in increasing order
of asymptotic (big-O) complexity:

\begin{problemparts}

\problempart \points{5} \textbf{Group 1:}

\[
\begin{array}{rcl}
f_{1}(n) & = & n^{0.999999}\log n\\
f_{2}(n) & = & 10000000n\\
f_{3}(n) & = & 1.000001^{n}\\
f_{4}(n) & = & n^{2}
\end{array}
\]

\ifsolution \solution{} 

With relatively simple functions, we can recognize the functions as
members of certain asymptotic classes, and automatically order them
according to class. For example, $f_{2}$ is linear in $n$, while
$f_{4}$ is polynomial, so $f_{2}=O(f_{4})$. Any exponential function
with base greater than one grows faster than any polynomial function,
so $f_{4}=O(f_{3}).$ Function $f_{1}$ is a logarithm times a function
that asymptotically approaches zero. It therefore grows more slowly
than a logarithm. Logarithms grow more slowly than linear functions,
so $f_{1}=O(f_{2})$. The asymptotic ordering is therefore $f_{1},f_{2},f_{3,}f_{4}$. 

\problempart \points{5} \textbf{Group 2:}

\[
\begin{array}{rcl}
f_{1}(n) & = & 2^{2^{1000000}}\\
f_{2}(n) & = & 2^{100000n}\\
f_{3}(n) & = & {\displaystyle \binom{n}{2}}\\
f_{4}(n) & = & n\sqrt{n}
\end{array}
\]

\ifsolution\solution{}

We'll continue to map functions to asymptotic classes. Function $f_{1}$
is an elaborately disguised constant. It's growth rate is zero. Function
$f_{2}$ is exponential. To see what class $f_{3}$ belongs to, we
can to transform it, as $\text{\ensuremath{\binom{N}{2}} = \ensuremath{\frac{N(N-1)}{2}} = O(\ensuremath{N^{2}})}$.
So $f_{3}$ is a second-degree polynomial. For $f_{4}$ we have $n\sqrt{n}=n^{\frac{3}{2}},$
so $f_{4}$ is a polynomial of order $\frac{3}{2}.$The asymptotic
ordering of these functions is therefore $f_{1},$$f_{4}$,$f_{3}$,
$f_{2}.$

\problempart \points{5} \textbf{Group 3:}

\paragraph{
\[
\protect\begin{array}{rcl}
f_{1}(n) & = & n^{\sqrt{n}}\protect\\
f_{2}(n) & = & 2^{n}\protect\\
f_{3}(n) & = & n^{10}\cdot2^{n/2}\protect\\
f_{4}(n) & = & {\displaystyle \sum_{i=1}^{n}(i+1)}
\protect\end{array}
\]
}

\ifsolution \solution{} 

If we cannot easily map functions to classes, we can calculate $\lim_{n\to\infty}\frac{f(n)}{g(n)}.$
If this limit is 0, then $g=O(f).$ To order $f_{1}$ and $f_{2}$,
write $\frac{f_{1}}{f_{2}}=\frac{n^{\sqrt{n}}}{2^{n}}=\frac{n^{\sqrt{n}}}{(2^{\sqrt{n}})^{\sqrt{n}}}=(\frac{n}{2^{\sqrt{n}}})^{\sqrt{n}}\to0$.
So $f_{1}=O(f_{2}).$ Comparing $f_{2}$ and $f_{3}$, we get $\frac{f_{2}}{f_{3}}=\frac{2^{n}}{n^{10}\cdot2^{n/2}}=\frac{2^{n/2}}{n^{10}}\to\infty$,
so $f_{3}=O(f_{2}).$ We have shown that both $f_{2}$ and $f_{3}$
grow more slowly than $f_{1}.$ 

The limit method is harder to apply to $f_{2}$ and $f_{3}$, so we
will instead rewrite them into a common form that will make them easier
to compare. Specifically, we will exponentiate both of them base 2.
For $f_{3}$ we get $n^{10}\cdot2^{n/2}=2^{\log_{2}(n^{10}\cdot2^{n/2})}=2^{10\cdot\log_{2}n+\frac{n}{2}}$.
For $f_{1}$ we get $2^{\log_{2}(n^{\sqrt{n}})}=2^{\sqrt{n}\cdot\log_{2}n)}.$The
exponent of $f_{1}$ is $O(\sqrt{n}\cdot\log_{2}n).$ The exponent
for $f_{3}$ is $O(n)$. Using the limit method on these functions,
we get $\frac{n}{\sqrt{n}\cdot\log_{2}n}=\frac{\sqrt{n}}{\log_{2}n}\to\infty$,
so $f_{3}=O(f_{1})$. Finally, $f_{4}(n)=\sum_{i}^{n}(i+1),$ which
is bounded above by $n^{2},$ so $f_{4}=O(n^{2})$. The asymptotic
order is therefore $f_{4},f_{2},f_{3,}f_{1}$. 

\end{problemparts}

\problem \points{15} \textbf{Recurrence Relation Resolution}

For each of the following recurrence relations, pick the correct asymptotic
runtime:

\begin{problemparts}

\problempart \points{5} Select the correct asymptotic complexity
of an algorithm with runtime $T(n,n)$ where 
\[
\begin{array}{rcll}
T(x,c) & = & \Theta(x) & \textrm{ for \ensuremath{c\le2}},\\
T(c,y) & = & \Theta(y) & \textrm{ for \ensuremath{c\le2}, and}\\
T(x,y) & = & \Theta(x+y)+T(x/2,y/2).
\end{array}
\]

\begin{enumerate}
\item $\Theta(\log n)$. 
\item $\Theta(n)$. $ $
\item $\Theta(n\log n)$. 
\item $\Theta(n\log^{2}n)$. 
\item $\Theta(n^{2})$. 
\item $\Theta(2^{n})$. 
\end{enumerate}
\ifsolution \solution{} %%% PROBLEM 2(a) SOLUTION START %%%
1 %%% PROBLEM 2(a) SOLUTION END %%%
\fi

\problempart \points{5} Select the correct asymptotic complexity
of an algorithm with runtime $T(n,n)$ where 
\[
\begin{array}{rcll}
T(x,c) & = & \Theta(x) & \textrm{ for \ensuremath{c\le2}},\\
T(c,y) & = & \Theta(y) & \textrm{ for \ensuremath{c\le2}, and}\\
T(x,y) & = & \Theta(x)+T(x,y/2).
\end{array}
\]

\begin{enumerate}
\item $\Theta(\log n)$. 
\item $\Theta(n)$. 
\item $\Theta(n\log n)$. 
\item $\Theta(n\log^{2}n)$. 
\item $\Theta(n^{2})$. 
\item $\Theta(2^{n})$. 
\end{enumerate}
\ifsolution \solution{} %%% PROBLEM 2(b) SOLUTION START %%%
1 %%% PROBLEM 2(b) SOLUTION END %%%
\fi

\problempart \points{5} Select the correct asymptotic complexity
of an algorithm with runtime $T(n,n)$ where 
\[
\begin{array}{rcll}
T(x,c) & = & \Theta(x) & \textrm{ for \ensuremath{c\le2}},\\
T(x,y) & = & \Theta(x)+S(x,y/2),\\
S(c,y) & = & \Theta(y) & \textrm{ for \ensuremath{c\le2}, and}\\
S(x,y) & = & \Theta(y)+T(x/2,y).
\end{array}
\]

\begin{enumerate}
\item $\Theta(\log n)$. 
\item $\Theta(n)$. 
\item $\Theta(n\log n)$. 
\item $\Theta(n\log^{2}n)$. 
\item $\Theta(n^{2})$. 
\item $\Theta(2^{n})$. 
\end{enumerate}
\ifsolution \solution{} %%% PROBLEM 2(c) SOLUTION START %%%
1 %%% PROBLEM 2(c) SOLUTION END %%%
\fi

\end{problemparts}

\section*{Peak-Finding}

In Lecture 1, you saw the peak-finding problem. As a reminder, a \emph{peak}
in a matrix is a location with the property that its four neighbors
(north, south, east, and west) have value less than or equal to the
value of the peak. We have posted Python code for solving this problem
to the website in a file called \texttt{ps1.zip}. In the file \texttt{algorithms.py},
there are four different algorithms which have been written to solve
the peak-finding problem, only some of which are correct. Your goal
is to figure out which of these algorithms are correct and which are
efficient.

\problem \points{16} \textbf{Peak-Finding Correctness}

\begin{problemparts}

\problempart \points{4} Is \texttt{algorithm1} correct? 
\begin{enumerate}
\item Yes. 
\item No. 
\end{enumerate}
\ifsolution \solution{} %%% PROBLEM 3(a) SOLUTION START %%%
1 %%% PROBLEM 3(a) SOLUTION END %%%
\fi

\problempart \points{4} Is \texttt{algorithm2} correct? 
\begin{enumerate}
\item Yes. 
\item No. 
\end{enumerate}
\ifsolution \solution{} %%% PROBLEM 3(b) SOLUTION START %%%
1 %%% PROBLEM 3(b) SOLUTION END %%%
\fi

\problempart \points{4} Is \texttt{algorithm3} correct? 
\begin{enumerate}
\item Yes. 
\item No. 
\end{enumerate}
\ifsolution \solution{} %%% PROBLEM 3(c) SOLUTION START %%%
1 %%% PROBLEM 3(c) SOLUTION END %%%
\fi

\problempart \points{4} Is \texttt{algorithm4} correct? 
\begin{enumerate}
\item Yes. 
\item No. 
\end{enumerate}
\ifsolution \solution{} %%% PROBLEM 3(d) SOLUTION START %%%
1 %%% PROBLEM 3(d) SOLUTION END %%%
\fi

\end{problemparts}

\problem \points{16} \textbf{Peak-Finding Efficiency}

\begin{problemparts}

\problempart \points{4} What is the worst-case runtime of \texttt{algorithm1}
on a problem of size $n\times n$? 
\begin{enumerate}
\item $\Theta(\log n)$. 
\item $\Theta(n)$. 
\item $\Theta(n\log n)$. 
\item $\Theta(n\log^{2}n)$. 
\item $\Theta(n^{2})$. 
\item $\Theta(2^{n})$. 
\end{enumerate}
\ifsolution \solution{} %%% PROBLEM 4(a) SOLUTION START %%%
1 %%% PROBLEM 4(a) SOLUTION END %%%
\fi

\problempart \points{4} What is the worst-case runtime of \texttt{algorithm2}
on a problem of size $n\times n$? 
\begin{enumerate}
\item $\Theta(\log n)$. 
\item $\Theta(n)$. 
\item $\Theta(n\log n)$. 
\item $\Theta(n\log^{2}n)$. 
\item $\Theta(n^{2})$. 
\item $\Theta(2^{n})$. 
\end{enumerate}
\ifsolution \solution{} %%% PROBLEM 4(b) SOLUTION START %%%
1 %%% PROBLEM 4(b) SOLUTION END %%%
\fi

\problempart \points{4} What is the worst-case runtime of \texttt{algorithm3}
on a problem of size $n\times n$? 
\begin{enumerate}
\item $\Theta(\log n)$. 
\item $\Theta(n)$. 
\item $\Theta(n\log n)$. 
\item $\Theta(n\log^{2}n)$. 
\item $\Theta(n^{2})$. 
\item $\Theta(2^{n})$. 
\end{enumerate}
\ifsolution \solution{} %%% PROBLEM 4(c) SOLUTION START %%%
1 %%% PROBLEM 4(c) SOLUTION END %%%
\fi

\problempart \points{4} What is the worst-case runtime of \texttt{algorithm4}
on a problem of size $n\times n$? 
\begin{enumerate}
\item $\Theta(\log n)$. 
\item $\Theta(n)$. 
\item $\Theta(n\log n)$. 
\item $\Theta(n\log^{2}n)$. 
\item $\Theta(n^{2})$. 
\item $\Theta(2^{n})$. 
\end{enumerate}
\ifsolution \solution{} %%% PROBLEM 4(d) SOLUTION START %%%
1 %%% PROBLEM 4(d) SOLUTION END %%%
\fi

\end{problemparts}

\problem \points{19} \textbf{Peak-Finding Proof}

Please modify the proof below to construct a proof of correctness
for the \emph{most efficient correct algorithm} among \texttt{algorithm2},
\texttt{algorithm3}, and \texttt{algorithm4}.

The following is the proof of correctness for \texttt{algorithm1},
which was sketched in Lecture 1.
\begin{quote}
We wish to show that \texttt{algorithm1} will always return a peak,
as long as the problem is not empty. To that end, we wish to prove
the following two statements:

\textbf{1. If the peak problem is not empty, then }\texttt{\textbf{algorithm1}}\textbf{
will always return a location.} Say that we start with a problem of
size $m\times n$. The recursive subproblem examined by \texttt{algorithm1}
will have dimensions $m\times\lfloor n/2\rfloor$ or $m\times\left(n-\lfloor n/2\rfloor-1\right)$.
Therefore, the number of columns in the problem strictly decreases
with each recursive call as long as $n>0$. So \texttt{algorithm1}
either returns a location at some point, or eventually examines a
subproblem with a non-positive number of columns. The only way for
the number of columns to become strictly negative, according to the
formulas that determine the size of the subproblem, is to have $n=0$
at some point. So if \texttt{algorithm1} doesn't return a location,
it must eventually examine an empty subproblem.

We wish to show that there is no way that this can occur. Assume,
to the contrary, that \texttt{algorithm1} does examine an empty subproblem.
Just prior to this, it must examine a subproblem of size $m\times1$
or $m\times2$. If the problem is of size $m\times1$, then calculating
the maximum of the central column is equivalent to calculating the
maximum of the entire problem. Hence, the maximum that the algorithm
finds must be a peak, and it will halt and return the location. If
the problem has dimensions $m\times2$, then there are two possibilities:
either the maximum of the central column is a peak (in which case
the algorithm will halt and return the location), or it has a strictly
better neighbor in the other column (in which case the algorithm will
recurse on the non-empty subproblem with dimensions $m\times1$, thus
reducing to the previous case). So \texttt{algorithm1} can never recurse
into an empty subproblem, and therefore \texttt{algorithm1} must eventually
return a location.

\textbf{2. If }\texttt{\textbf{algorithm1}}\textbf{ returns a location,
it will be a peak in the original problem.} If \texttt{algorithm1}
returns a location $(r_{1},c_{1})$, then that location must have
the best value in column $c_{1}$, and must have been a peak within
some recursive subproblem. Assume, for the sake of contradiction,
that $(r_{1},c_{1})$ is not also a peak within the original problem.
Then as the location $(r_{1},c_{1})$ is passed up the chain of recursive
calls, it must eventually reach a level where it stops being a peak.
At that level, the location $(r_{1},c_{1})$ must be adjacent to the
dividing column $c_{2}$ (where $|c_{1}-c_{2}|=1$), and the values
must satisfy the inequality $val(r_{1},c_{1})<val(r_{1},c_{2})$.

Let $(r_{2},c_{2})$ be the location of the maximum value found by
\texttt{algorithm1} in the dividing column. As a result, it must be
that $val(r_{1},c_{2})\le val(r_{2},c_{2})$. Because the algorithm
chose to recurse on the half containing $(r_{1},c_{1})$, we know
that $val(r_{2},c_{2})<val(r_{2},c_{1})$. Hence, we have the following
chain of inequalities: 
\[
val(r_{1},c_{1})<val(r_{1},c_{2})\le val(r_{2},c_{2})<val(r_{2},c_{1})
\]
But in order for \texttt{algorithm1} to return $(r_{1},c_{1})$ as
a peak, the value at $(r_{1},c_{1})$ must have been the greatest
in its column, making $val(r_{1},c_{1})\ge val(r_{2},c_{1})$. Hence,
we have a contradiction. 
\end{quote}
\ifsolution \solution{} %%% PROBLEM 5 SOLUTION START %%%
Write your proof here. %%% PROBLEM 5 SOLUTION END %%%
\fi

\problem \points{19} \textbf{Peak-Finding Counterexamples}

For each incorrect algorithm, upload a Python file giving a counterexample
(i.e. a matrix for which the algorithm returns a location that is
not a peak).

\ifsolution \solution{} %%% PROBLEM 6 SOLUTION START %%%

\begin{verbatim}

problemMatrix = [
    [0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0]
]

problemMatrix = [
    [0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0]
]
\end{verbatim}
%%% PROBLEM 6 SOLUTION END %%%
\fi

\end{problems}
\end{document}
